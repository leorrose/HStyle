{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYMYk6yPUv79"
      },
      "source": [
        "# Historical style generator\n",
        "##### Students – Leor Ariel Rose, Yahav Bar David\n",
        "##### Academic advisor - Dr. Irina Rabaev\n",
        "\n",
        "This notebook contains our document style tranfer model and the explanations of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kGoL1qs8qWt"
      },
      "source": [
        "First we will import all necessary libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPY561rZa7H"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import List, Union, Tuple, Dict\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from PIL import Image\n",
        "from shutil import copyfile\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OhtdZeyp7hm"
      },
      "source": [
        "Next lets mount our drive with our data and folders to save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vRBOLFlp8I5",
        "outputId": "be778fc8-2548-4b52-cd67-4b0b5a41f7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-xNBMq8ZkS7"
      },
      "source": [
        "Next we will define our document style transfer class (every method and piece of code is explained by comments and docstrings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMbymj86AoDv"
      },
      "source": [
        "class DocumentStyleTransfer():\n",
        "    totalVariationWeight:float = 0\n",
        "    styleWeight:float = 0\n",
        "    contentWeight:float = 0\n",
        "    imgNumRows:int = 0\n",
        "    imgNumCols:int = 0\n",
        "    ftm:tf.keras.Model = None\n",
        "\n",
        "    def _preprocessImage(self, imagePath:str) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Private Method to open, resize and format pictures into appropriate tensors\n",
        "\n",
        "        Args:\n",
        "            image_path (str): the path to image\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: a tensor representing image\n",
        "        \"\"\"\n",
        "        # load image with content image size\n",
        "        img:Image.Image  = keras.preprocessing.image.load_img(imagePath, target_size=(self.imgNumRows, self.imgNumCols))\n",
        "        # create numpy array of image\n",
        "        img:np.ndarray = keras.preprocessing.image.img_to_array(img)\n",
        "        # expand to 3d for model\n",
        "        img:np.ndarray = np.expand_dims(img, axis=0)\n",
        "        # convert from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
        "        img:Union[np.ndarray,tf.Tensor] = vgg19.preprocess_input(img)\n",
        "        return tf.convert_to_tensor(img)\n",
        "\n",
        "\n",
        "    def _deprocessImage(self, tns:tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Private Method to convert a tensor into a valid image\n",
        "\n",
        "        Args:\n",
        "            tns (tf.Tensor): a tensor from model output\n",
        "\n",
        "        Returns:\n",
        "            [tf.Tensor]: a tensor that represents an image\n",
        "        \"\"\"\n",
        "        # reshape to 2d instead of 3d from model output: '(1, 400, 571, 3)' -> '(400, 571, 3)'\n",
        "        tns:tf.tensor = tns.reshape((self.imgNumRows, self.imgNumCols, 3))\n",
        "                \n",
        "        # Remove zero-center by mean pixel\n",
        "        tns[:, :, 0] += 103.939\n",
        "        tns[:, :, 1] += 116.779\n",
        "        tns[:, :, 2] += 123.68\n",
        "                \n",
        "        # 'BGR'->'RGB'\n",
        "        tns:tf.tensor = tns[:, :, ::-1]\n",
        "        tns:tf.tensor = np.clip(tns, 0, 255).astype(\"uint8\")\n",
        "                \n",
        "        # return valid tensor image\n",
        "        return tns\n",
        "\n",
        "    def _gramMatrix(self, tns:tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Private Method to calculate gram matrix of an image tensor (feature-wise outer product)\n",
        "\n",
        "        Args:\n",
        "            tns (tf.Tensor): a tensor that represents an image\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: the _gramMatrix result\n",
        "        \"\"\"\n",
        "        # transpose tensor by (2,0,1) for example tensor shape (400, 571, 64)' -> '(64, 400, 571)'\n",
        "        tns:tf.tensor = tf.transpose(tns, (2, 0, 1))\n",
        "        # flatten to 2d for example tensor shape '(64, 400, 571)' -> '(64, 228400)'\n",
        "        features:tf.tensor = tf.reshape(tns, (tf.shape(tns)[0], -1))\n",
        "        # matrix multiplication of features and features transpose\n",
        "        gram:tf.tensor = tf.matmul(features, tf.transpose(features))\n",
        "        return gram\n",
        "\n",
        "    def _styleLoss(self, style:tf.Tensor, combination:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Method to calculate the style loss. \n",
        "        The \"style loss\" is designed to maintain the style of the reference image in the generated image.\n",
        "        It is contentd on the gram matrices (which capture style) of feature maps from the style reference image\n",
        "        and from the generated image.\n",
        "\n",
        "        Args:\n",
        "            style (tf.Tensor): tensor representing style image\n",
        "            combination (tf.Tensor): tensor representing combination image\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        # calc gram matrix for style\n",
        "        style_gramMatrix:tf.tensor = self._gramMatrix(style)\n",
        "        # calc gram matrix for combination\n",
        "        combination_gramMatrix:tf.tensor = self._gramMatrix(combination)\n",
        "        # image depth\n",
        "        channels:int = 3\n",
        "        # image size\n",
        "        size:int = self.imgNumRows * self.imgNumCols\n",
        "        # MSE loss between gram matrix of input and the style image\n",
        "        return tf.reduce_sum(tf.square(style_gramMatrix - combination_gramMatrix)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "\n",
        "    def _contentLoss(self, content:tf.Tensor, combination:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Method to calculate the content loss. \n",
        "        An auxiliary loss function designed to maintain the \"content\" of the\n",
        "        content image in the generated image.\n",
        "\n",
        "        Args:\n",
        "            content (tf.Tensor): tensor representing the content content image\n",
        "            combination (tf.Tensor): tensor representing combination image\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        # MSE loss between the content content image’s features and the combination image’s features\n",
        "        return tf.reduce_sum(tf.square(combination - content))\n",
        "        \n",
        "    def _totalVariationLoss(self, tns:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Methos to calculate total variation loss (a regularization loss), designed to keep the generated image locally coherent.\n",
        "\n",
        "        Args:\n",
        "            tns (tf.Tensor):  tensor representing the generated image\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        a:tf.Tensor = tf.square(tns[:, : self.imgNumRows - 1, : self.imgNumCols - 1, :] - tns[:, 1:, : self.imgNumCols - 1, :])\n",
        "        b:tf.Tensor = tf.square(tns[:, : self.imgNumRows - 1, : self.imgNumCols - 1, :] - tns[:, : self.imgNumRows - 1, 1:, :])\n",
        "        return tf.reduce_sum(tf.pow(a + b, 1.25))\n",
        "\n",
        "    def _featureExtractorModel(self) -> tf.keras.Model:\n",
        "        \"\"\"\n",
        "        Private Method to create a model that returns the activation values for every layer in VGG19 (as a dict)\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: a model that returns the activation values for every layer in VGG19 (as a dict)\n",
        "        \"\"\"\n",
        "        # Build a VGG19 model loaded with pre-trained ImageNet weights\n",
        "        model:tf.keras.Model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "        # Get the symbolic outputs of each \"key\" layer (they have unique names)\n",
        "        outputsDict:Dict[str, tf.Tensor] = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "\n",
        "        # Set up a model that returns the activation values for every layer in VGG19 (as a dict).\n",
        "        featureExtractor:tf.keras.Model = keras.Model(inputs=model.inputs, outputs=outputsDict)\n",
        "        return featureExtractor\n",
        "\n",
        "    def _computeLoss(self, combinationImage:tf.Tensor, contentImage:tf.Tensor, styleReferenceImage:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Method to combine style, content, total variation loss functions  into one loss function for model evaluation.\n",
        "\n",
        "        Args:\n",
        "            combinationImage (tf.Tensor): \n",
        "            contentImage (tf.Tensor): \n",
        "            styleReferenceImage (tf.Tensor):\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        # List of layers to use for the style loss.\n",
        "        styleLayerNames:List[str] = [\"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\", \"block5_conv1\"]\n",
        "        \n",
        "        # The layer to use for the content loss.\n",
        "        contentLayerName:str = \"block5_conv2\"\n",
        "\n",
        "        # concat all images to one for model prediction\n",
        "        inputTensor:tf.Tensor = tf.concat([contentImage, styleReferenceImage, combinationImage], axis=0)\n",
        "            \n",
        "        # get prediction from model with input image as a dict of each layer\n",
        "        features:Dict[str, tf.Tensor]  = self.ftm(inputTensor)\n",
        "\n",
        "        # Initialize the loss\n",
        "        loss:np.ndarray = tf.zeros(shape=())\n",
        "\n",
        "        # get content layer\n",
        "        layerFeatures = features[contentLayerName] \n",
        "        # get content image features from layer prediction\n",
        "        contentImageFeatures = layerFeatures[0, :, :, :]\n",
        "        # get combination image features from layer prediction\n",
        "        combinationFeatures = layerFeatures[2, :, :, :]\n",
        "        # add content loss to total loss\n",
        "        loss:np.ndarray = loss + self.contentWeight * self._contentLoss(contentImageFeatures, combinationFeatures)\n",
        "\n",
        "        # Add style loss\n",
        "        for layerName in styleLayerNames:\n",
        "            # get style layer \n",
        "            layerFeatures = features[layerName]\n",
        "            # get style image features from layer prediction\n",
        "            styleReferenceFeatures = layerFeatures[1, :, :, :]\n",
        "            # get combination image features from layer prediction\n",
        "            combinationFeatures = layerFeatures[2, :, :, :]\n",
        "            # add style loss to total loss\n",
        "            loss += (self.styleWeight / len(styleLayerNames)) * self._styleLoss(styleReferenceFeatures, combinationFeatures)\n",
        "\n",
        "        # Add total variation loss\n",
        "        # loss += self.totalVariationWeight * self._totalVariationLoss(combinationImage)\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def _computeLossAndGrads(self, combinationImage:tf.Tensor, contentImage:tf.Tensor, styleReferenceImage:tf.Tensor) -> Tuple[float, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Private Method to compile loss function to make it faster.\n",
        "\n",
        "        Args:\n",
        "            combinationImage (tf.Tensor): \n",
        "            contentImage (tf.Tensor): \n",
        "            styleReferenceImage (tf.Tensor): \n",
        "\n",
        "        Returns:\n",
        "            Tuple[float, tf.Tensor]: \n",
        "        \"\"\"\n",
        "        # \n",
        "        with tf.GradientTape() as tape:\n",
        "            #\n",
        "            loss:float = self._computeLoss(combinationImage, contentImage, styleReferenceImage)\n",
        "        #\n",
        "        grads:tf.Tensor = tape.gradient(loss, combinationImage)\n",
        "        return loss, grads\n",
        "\n",
        "    def renderImage(self, contentImgPath:str, styleImgPath:str, resultPrefix:str = 'result', resultsDir:str = './', \n",
        "                    totalVariationWeight:float = 1e-6, styleWeight:float = 1e-6, contentWeight:float = 2.5e-8, iterSave:int = 4000) -> None:\n",
        "        \"\"\"\n",
        "        Method to render a style transfer image from content and style\n",
        "\n",
        "        Args:\n",
        "            contentImgPath (str): our content image path\n",
        "            styleImgPath (str): our style image path\n",
        "            resultPrefix (str, optional): the result prefix for saving images and content. Defaults to 'result-'.\n",
        "            resultsDir (str, optional): directory to save content. Defaults to './'.\n",
        "            totalVariationWeight (float, optional): total variation loss weight. Defaults to 1e-6.\n",
        "            styleWeight (float, optional): style loss weight. Defaults to 1e-6.\n",
        "            contentWeight (float, optional): content loss weight. Defaults to 2.5e-8.\n",
        "            iterSave (int, optional): iteration difference to save image (each itersave it will save result image). Defaults to 4000 (one image).\n",
        "        \"\"\"\n",
        "        # create directory for results\n",
        "        if os.path.exists(resultsDir):\n",
        "            shutil.rmtree(resultsDir)\n",
        "        os.mkdir(resultsDir)\n",
        "\n",
        "        # Weights of the different loss components\n",
        "        self.totalVariationWeight:float = totalVariationWeight\n",
        "        self.styleWeight:float = styleWeight\n",
        "        self.contentWeight:float = contentWeight\n",
        "\n",
        "        # Dimensions of the generated picture.\n",
        "        width, height = keras.preprocessing.image.load_img(contentImgPath).size\n",
        "        self.imgNumCols:int = 400\n",
        "        self.imgNumRows:int = int(width * self.imgNumCols / height)\n",
        "\n",
        "        # create feature extractor model\n",
        "        self.ftm = self._featureExtractorModel()\n",
        "\n",
        "        # set model optimizer\n",
        "        optimizer = keras.optimizers.SGD(keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96))\n",
        "\n",
        "        # get content image\n",
        "        contentImage = self._preprocessImage(contentImgPath)\n",
        "        # get style image\n",
        "        styleReferenceImage = self._preprocessImage(styleImgPath)\n",
        "        # create combination initial image\n",
        "        combinationImage = tf.Variable(self._preprocessImage(contentImgPath))\n",
        "\n",
        "        # set number of iteration of model\n",
        "        iterations = 4000\n",
        "        for i in range(1, iterations + 1):\n",
        "            # compute loss and gradient\n",
        "            loss, grads = self._computeLossAndGrads(combinationImage, contentImage, styleReferenceImage)\n",
        "            # apply the gradient to combination image\n",
        "            optimizer.apply_gradients([(grads, combinationImage)])\n",
        "            # for each 50 iteration ouput results\n",
        "            if i % iterSave == 0:\n",
        "                print(f\"iteration {i} loss: {loss}\")\n",
        "                img = self._deprocessImage(combinationImage.numpy())\n",
        "                fname = resultsDir + '/' + resultPrefix + f\"_at_iteration_{i}.png\"\n",
        "                keras.preprocessing.image.save_img(fname, img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4kL99NJaB50"
      },
      "source": [
        "Next we will define our content and loss values in order to keep a raitio between them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y08zXmdRaLu-"
      },
      "source": [
        "# weights\n",
        "contentWeights: List[float] = [1.00E-06, 1.00E-07, 1.00E-08, 1.00E-09, 1.00E-10, 1.00E-06, 1.00E-06, 1.00E-06, 1.00E-06]\n",
        "styleWeights: List[float] =   [1.00E-06, 1.00E-06, 1.00E-06, 1.00E-06, 1.00E-06, 1.00E-07, 1.00E-08, 1.00E-09, 1.00E-10]\n",
        "totalVariationWeight:float = 1e-06"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZp6rz_iaU4i"
      },
      "source": [
        "Lets create a document style transfer object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNABikukaXc5"
      },
      "source": [
        "# model\n",
        "model:DocumentStyleTransfer = DocumentStyleTransfer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUZbc-qfaYEA"
      },
      "source": [
        "And know for each content and style loss we will apply document style transfer and save the result and our research parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0p3OALAA6_Y",
        "outputId": "2e9622cf-92b3-4832-f51b-00a20c48061d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "content_path = '/content/content.png' \n",
        "style_path = '/content/style.png' \n",
        "experements_path = '/content/drive/My Drive/final_project/experiments/hebrem_to_hebrew_middle_ages/full_content_document/style_document_without_text'\n",
        "experement_number = 1\n",
        "\n",
        "\n",
        "# experements\n",
        "for i in range(0, len(contentWeights)):\n",
        "  \n",
        "  experement_dir = f\"{experements_path}/test {experement_number}/experement{i}\"\n",
        "  \n",
        "  os.makedirs(experement_dir)\n",
        "\n",
        "  model.renderImage(content_path, style_path, f\"expererment_{i}\", experement_dir, totalVariationWeight, styleWeights[i], contentWeights[i], 50)\n",
        "  \n",
        "  # create read me of the experement inside the folder\n",
        "  with open(f\"{experement_dir}/readme.txt\", 'w') as readme:\n",
        "    readme.write(f\"experementNumber = {i}\\n\")\n",
        "    readme.write(f\"from = modern hebrew\\n\")\n",
        "    readme.write(f\"to = middle ages hebrew\\n\")\n",
        "    # readme.write(f\"totalVariationWeight = {totalVariationWeight}\\n\")\n",
        "    readme.write(f\"styleWeight = {styleWeights[i]}\\n\")\n",
        "    readme.write(f\"contentWeight = {contentWeights[i]}\\n\")\n",
        "  \n",
        "  # add contetnt and style to experement folder   \n",
        "  copyfile('/content/content.png', f\"{experement_dir}/content.png\")\n",
        "  copyfile('/content/style.png', f\"{experement_dir}/style.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 50 loss: 6429.6611328125\n",
            "iteration 100 loss: 4741.64892578125\n",
            "iteration 150 loss: 3985.11279296875\n",
            "iteration 200 loss: 3530.953125\n",
            "iteration 250 loss: 3219.5830078125\n",
            "iteration 300 loss: 2987.418701171875\n",
            "iteration 350 loss: 2806.294189453125\n",
            "iteration 400 loss: 2660.03125\n",
            "iteration 450 loss: 2538.550048828125\n",
            "iteration 500 loss: 2435.384765625\n",
            "iteration 550 loss: 2345.7529296875\n",
            "iteration 600 loss: 2267.0791015625\n",
            "iteration 650 loss: 2197.295166015625\n",
            "iteration 700 loss: 2134.918212890625\n",
            "iteration 750 loss: 2078.7177734375\n",
            "iteration 800 loss: 2027.893310546875\n",
            "iteration 850 loss: 1981.5072021484375\n",
            "iteration 900 loss: 1939.001708984375\n",
            "iteration 950 loss: 1900.09130859375\n",
            "iteration 1000 loss: 1864.2237548828125\n",
            "iteration 1050 loss: 1830.9052734375\n",
            "iteration 1100 loss: 1799.9063720703125\n",
            "iteration 1150 loss: 1771.031005859375\n",
            "iteration 1200 loss: 1744.0618896484375\n",
            "iteration 1250 loss: 1718.8074951171875\n",
            "iteration 1300 loss: 1694.9873046875\n",
            "iteration 1350 loss: 1672.5731201171875\n",
            "iteration 1400 loss: 1651.430908203125\n",
            "iteration 1450 loss: 1631.4627685546875\n",
            "iteration 1500 loss: 1612.5537109375\n",
            "iteration 1550 loss: 1594.6533203125\n",
            "iteration 1600 loss: 1577.7147216796875\n",
            "iteration 1650 loss: 1561.6697998046875\n",
            "iteration 1700 loss: 1546.36962890625\n",
            "iteration 1750 loss: 1531.7891845703125\n",
            "iteration 1800 loss: 1517.8897705078125\n",
            "iteration 1850 loss: 1504.620361328125\n",
            "iteration 1900 loss: 1491.915771484375\n",
            "iteration 1950 loss: 1479.740478515625\n",
            "iteration 2000 loss: 1468.0792236328125\n",
            "iteration 2050 loss: 1456.90673828125\n",
            "iteration 2100 loss: 1446.185791015625\n",
            "iteration 2150 loss: 1435.8935546875\n",
            "iteration 2200 loss: 1426.0086669921875\n",
            "iteration 2250 loss: 1416.5152587890625\n",
            "iteration 2300 loss: 1407.3853759765625\n",
            "iteration 2350 loss: 1398.5948486328125\n",
            "iteration 2400 loss: 1390.128662109375\n",
            "iteration 2450 loss: 1381.961669921875\n",
            "iteration 2500 loss: 1374.08349609375\n",
            "iteration 2550 loss: 1366.48828125\n",
            "iteration 2600 loss: 1359.1553955078125\n",
            "iteration 2650 loss: 1352.0699462890625\n",
            "iteration 2700 loss: 1345.2119140625\n",
            "iteration 2750 loss: 1338.5865478515625\n",
            "iteration 2800 loss: 1332.1844482421875\n",
            "iteration 2850 loss: 1326.00439453125\n",
            "iteration 2900 loss: 1320.0233154296875\n",
            "iteration 2950 loss: 1314.232421875\n",
            "iteration 3000 loss: 1308.6239013671875\n",
            "iteration 3050 loss: 1303.1903076171875\n",
            "iteration 3100 loss: 1297.921875\n",
            "iteration 3150 loss: 1292.8125\n",
            "iteration 3200 loss: 1287.861328125\n",
            "iteration 3250 loss: 1283.05908203125\n",
            "iteration 3300 loss: 1278.392822265625\n",
            "iteration 3350 loss: 1273.863037109375\n",
            "iteration 3400 loss: 1269.4700927734375\n",
            "iteration 3450 loss: 1265.21240234375\n",
            "iteration 3500 loss: 1261.0809326171875\n",
            "iteration 3550 loss: 1257.0655517578125\n",
            "iteration 3600 loss: 1253.161865234375\n",
            "iteration 3650 loss: 1249.3702392578125\n",
            "iteration 3700 loss: 1245.68701171875\n",
            "iteration 3750 loss: 1242.1055908203125\n",
            "iteration 3800 loss: 1238.61962890625\n",
            "iteration 3850 loss: 1235.23193359375\n",
            "iteration 3900 loss: 1231.937744140625\n",
            "iteration 3950 loss: 1228.7340087890625\n",
            "iteration 4000 loss: 1225.6138916015625\n",
            "iteration 50 loss: 6104.32958984375\n",
            "iteration 100 loss: 4420.384765625\n",
            "iteration 150 loss: 3662.8935546875\n",
            "iteration 200 loss: 3209.154296875\n",
            "iteration 250 loss: 2897.513916015625\n",
            "iteration 300 loss: 2665.98583984375\n",
            "iteration 350 loss: 2485.063232421875\n",
            "iteration 400 loss: 2338.754150390625\n",
            "iteration 450 loss: 2217.111083984375\n",
            "iteration 500 loss: 2114.14208984375\n",
            "iteration 550 loss: 2025.53369140625\n",
            "iteration 600 loss: 1948.3519287109375\n",
            "iteration 650 loss: 1880.5107421875\n",
            "iteration 700 loss: 1820.1512451171875\n",
            "iteration 750 loss: 1766.208984375\n",
            "iteration 800 loss: 1717.6573486328125\n",
            "iteration 850 loss: 1673.6224365234375\n",
            "iteration 900 loss: 1633.427978515625\n",
            "iteration 950 loss: 1596.4837646484375\n",
            "iteration 1000 loss: 1562.51904296875\n",
            "iteration 1050 loss: 1531.179443359375\n",
            "iteration 1100 loss: 1502.1048583984375\n",
            "iteration 1150 loss: 1475.1220703125\n",
            "iteration 1200 loss: 1449.9815673828125\n",
            "iteration 1250 loss: 1426.4937744140625\n",
            "iteration 1300 loss: 1404.487060546875\n",
            "iteration 1350 loss: 1383.839599609375\n",
            "iteration 1400 loss: 1364.3955078125\n",
            "iteration 1450 loss: 1346.09814453125\n",
            "iteration 1500 loss: 1328.805908203125\n",
            "iteration 1550 loss: 1312.4222412109375\n",
            "iteration 1600 loss: 1296.89208984375\n",
            "iteration 1650 loss: 1282.153564453125\n",
            "iteration 1700 loss: 1268.1422119140625\n",
            "iteration 1750 loss: 1254.80908203125\n",
            "iteration 1800 loss: 1242.1072998046875\n",
            "iteration 1850 loss: 1229.9876708984375\n",
            "iteration 1900 loss: 1218.4063720703125\n",
            "iteration 1950 loss: 1207.3370361328125\n",
            "iteration 2000 loss: 1196.750244140625\n",
            "iteration 2050 loss: 1186.6190185546875\n",
            "iteration 2100 loss: 1176.8988037109375\n",
            "iteration 2150 loss: 1167.5821533203125\n",
            "iteration 2200 loss: 1158.63623046875\n",
            "iteration 2250 loss: 1150.044677734375\n",
            "iteration 2300 loss: 1141.785888671875\n",
            "iteration 2350 loss: 1133.8341064453125\n",
            "iteration 2400 loss: 1126.1715087890625\n",
            "iteration 2450 loss: 1118.786376953125\n",
            "iteration 2500 loss: 1111.6689453125\n",
            "iteration 2550 loss: 1104.80029296875\n",
            "iteration 2600 loss: 1098.1724853515625\n",
            "iteration 2650 loss: 1091.7684326171875\n",
            "iteration 2700 loss: 1085.577392578125\n",
            "iteration 2750 loss: 1079.589111328125\n",
            "iteration 2800 loss: 1073.8050537109375\n",
            "iteration 2850 loss: 1068.203369140625\n",
            "iteration 2900 loss: 1062.7850341796875\n",
            "iteration 2950 loss: 1057.53955078125\n",
            "iteration 3000 loss: 1052.4591064453125\n",
            "iteration 3050 loss: 1047.5413818359375\n",
            "iteration 3100 loss: 1042.7781982421875\n",
            "iteration 3150 loss: 1038.1614990234375\n",
            "iteration 3200 loss: 1033.6873779296875\n",
            "iteration 3250 loss: 1029.349853515625\n",
            "iteration 3300 loss: 1025.139404296875\n",
            "iteration 3350 loss: 1021.0525512695312\n",
            "iteration 3400 loss: 1017.0852661132812\n",
            "iteration 3450 loss: 1013.2298583984375\n",
            "iteration 3500 loss: 1009.4847412109375\n",
            "iteration 3550 loss: 1005.8468627929688\n",
            "iteration 3600 loss: 1002.31396484375\n",
            "iteration 3650 loss: 998.8806762695312\n",
            "iteration 3700 loss: 995.5429077148438\n",
            "iteration 3750 loss: 992.2944946289062\n",
            "iteration 3800 loss: 989.1354370117188\n",
            "iteration 3850 loss: 986.064453125\n",
            "iteration 3900 loss: 983.077392578125\n",
            "iteration 3950 loss: 980.1702270507812\n",
            "iteration 4000 loss: 977.3422241210938\n",
            "iteration 50 loss: 6065.701171875\n",
            "iteration 100 loss: 4380.72705078125\n",
            "iteration 150 loss: 3621.87841796875\n",
            "iteration 200 loss: 3167.01318359375\n",
            "iteration 250 loss: 2854.481201171875\n",
            "iteration 300 loss: 2621.46728515625\n",
            "iteration 350 loss: 2439.2900390625\n",
            "iteration 400 loss: 2291.825439453125\n",
            "iteration 450 loss: 2169.58740234375\n",
            "iteration 500 loss: 2066.124755859375\n",
            "iteration 550 loss: 1977.2144775390625\n",
            "iteration 600 loss: 1900.049560546875\n",
            "iteration 650 loss: 1831.968505859375\n",
            "iteration 700 loss: 1771.435302734375\n",
            "iteration 750 loss: 1717.301513671875\n",
            "iteration 800 loss: 1668.5469970703125\n",
            "iteration 850 loss: 1624.289306640625\n",
            "iteration 900 loss: 1583.97607421875\n",
            "iteration 950 loss: 1547.106201171875\n",
            "iteration 1000 loss: 1513.1781005859375\n",
            "iteration 1050 loss: 1481.894287109375\n",
            "iteration 1100 loss: 1452.90478515625\n",
            "iteration 1150 loss: 1425.944580078125\n",
            "iteration 1200 loss: 1400.85009765625\n",
            "iteration 1250 loss: 1377.3524169921875\n",
            "iteration 1300 loss: 1355.38037109375\n",
            "iteration 1350 loss: 1334.78955078125\n",
            "iteration 1400 loss: 1315.443115234375\n",
            "iteration 1450 loss: 1297.2032470703125\n",
            "iteration 1500 loss: 1279.9725341796875\n",
            "iteration 1550 loss: 1263.7034912109375\n",
            "iteration 1600 loss: 1248.306640625\n",
            "iteration 1650 loss: 1233.6922607421875\n",
            "iteration 1700 loss: 1219.811767578125\n",
            "iteration 1750 loss: 1206.60107421875\n",
            "iteration 1800 loss: 1194.0184326171875\n",
            "iteration 1850 loss: 1182.00927734375\n",
            "iteration 1900 loss: 1170.53076171875\n",
            "iteration 1950 loss: 1159.564697265625\n",
            "iteration 2000 loss: 1149.0654296875\n",
            "iteration 2050 loss: 1138.99755859375\n",
            "iteration 2100 loss: 1129.340576171875\n",
            "iteration 2150 loss: 1120.0831298828125\n",
            "iteration 2200 loss: 1111.2042236328125\n",
            "iteration 2250 loss: 1102.6697998046875\n",
            "iteration 2300 loss: 1094.4703369140625\n",
            "iteration 2350 loss: 1086.582763671875\n",
            "iteration 2400 loss: 1078.9898681640625\n",
            "iteration 2450 loss: 1071.67236328125\n",
            "iteration 2500 loss: 1064.6080322265625\n",
            "iteration 2550 loss: 1057.7923583984375\n",
            "iteration 2600 loss: 1051.220458984375\n",
            "iteration 2650 loss: 1044.87744140625\n",
            "iteration 2700 loss: 1038.7493896484375\n",
            "iteration 2750 loss: 1032.82568359375\n",
            "iteration 2800 loss: 1027.1002197265625\n",
            "iteration 2850 loss: 1021.5607299804688\n",
            "iteration 2900 loss: 1016.2022705078125\n",
            "iteration 2950 loss: 1011.016845703125\n",
            "iteration 3000 loss: 1005.9943237304688\n",
            "iteration 3050 loss: 1001.1249389648438\n",
            "iteration 3100 loss: 996.4057006835938\n",
            "iteration 3150 loss: 991.8328247070312\n",
            "iteration 3200 loss: 987.3966064453125\n",
            "iteration 3250 loss: 983.0906982421875\n",
            "iteration 3300 loss: 978.9142456054688\n",
            "iteration 3350 loss: 974.8600463867188\n",
            "iteration 3400 loss: 970.9224243164062\n",
            "iteration 3450 loss: 967.096923828125\n",
            "iteration 3500 loss: 963.3803100585938\n",
            "iteration 3550 loss: 959.7691040039062\n",
            "iteration 3600 loss: 956.26220703125\n",
            "iteration 3650 loss: 952.8558349609375\n",
            "iteration 3700 loss: 949.5430908203125\n",
            "iteration 3750 loss: 946.321044921875\n",
            "iteration 3800 loss: 943.1880493164062\n",
            "iteration 3850 loss: 940.1411743164062\n",
            "iteration 3900 loss: 937.1771240234375\n",
            "iteration 3950 loss: 934.2939453125\n",
            "iteration 4000 loss: 931.488525390625\n",
            "iteration 50 loss: 6061.716796875\n",
            "iteration 100 loss: 4376.92822265625\n",
            "iteration 150 loss: 3617.1865234375\n",
            "iteration 200 loss: 3161.793701171875\n",
            "iteration 250 loss: 2848.88037109375\n",
            "iteration 300 loss: 2616.033935546875\n",
            "iteration 350 loss: 2434.377197265625\n",
            "iteration 400 loss: 2287.25146484375\n",
            "iteration 450 loss: 2165.34423828125\n",
            "iteration 500 loss: 2061.7412109375\n",
            "iteration 550 loss: 1972.589599609375\n",
            "iteration 600 loss: 1894.9185791015625\n",
            "iteration 650 loss: 1826.5732421875\n",
            "iteration 700 loss: 1766.0166015625\n",
            "iteration 750 loss: 1711.86474609375\n",
            "iteration 800 loss: 1663.2674560546875\n",
            "iteration 850 loss: 1619.2362060546875\n",
            "iteration 900 loss: 1579.0260009765625\n",
            "iteration 950 loss: 1542.212890625\n",
            "iteration 1000 loss: 1508.361083984375\n",
            "iteration 1050 loss: 1477.1292724609375\n",
            "iteration 1100 loss: 1448.24658203125\n",
            "iteration 1150 loss: 1421.38427734375\n",
            "iteration 1200 loss: 1396.3265380859375\n",
            "iteration 1250 loss: 1372.915771484375\n",
            "iteration 1300 loss: 1350.958984375\n",
            "iteration 1350 loss: 1330.3887939453125\n",
            "iteration 1400 loss: 1311.0311279296875\n",
            "iteration 1450 loss: 1292.7784423828125\n",
            "iteration 1500 loss: 1275.55029296875\n",
            "iteration 1550 loss: 1259.25634765625\n",
            "iteration 1600 loss: 1243.822265625\n",
            "iteration 1650 loss: 1229.16943359375\n",
            "iteration 1700 loss: 1215.2364501953125\n",
            "iteration 1750 loss: 1201.9769287109375\n",
            "iteration 1800 loss: 1189.3477783203125\n",
            "iteration 1850 loss: 1177.2939453125\n",
            "iteration 1900 loss: 1165.7860107421875\n",
            "iteration 1950 loss: 1154.7928466796875\n",
            "iteration 2000 loss: 1144.274169921875\n",
            "iteration 2050 loss: 1134.20361328125\n",
            "iteration 2100 loss: 1124.541015625\n",
            "iteration 2150 loss: 1115.2689208984375\n",
            "iteration 2200 loss: 1106.3665771484375\n",
            "iteration 2250 loss: 1097.8206787109375\n",
            "iteration 2300 loss: 1089.593505859375\n",
            "iteration 2350 loss: 1081.6766357421875\n",
            "iteration 2400 loss: 1074.0557861328125\n",
            "iteration 2450 loss: 1066.7255859375\n",
            "iteration 2500 loss: 1059.6654052734375\n",
            "iteration 2550 loss: 1052.8592529296875\n",
            "iteration 2600 loss: 1046.284912109375\n",
            "iteration 2650 loss: 1039.9283447265625\n",
            "iteration 2700 loss: 1033.7906494140625\n",
            "iteration 2750 loss: 1027.859130859375\n",
            "iteration 2800 loss: 1022.1282348632812\n",
            "iteration 2850 loss: 1016.5866088867188\n",
            "iteration 2900 loss: 1011.2297973632812\n",
            "iteration 2950 loss: 1006.0462036132812\n",
            "iteration 3000 loss: 1001.02490234375\n",
            "iteration 3050 loss: 996.1607055664062\n",
            "iteration 3100 loss: 991.4464111328125\n",
            "iteration 3150 loss: 986.8773803710938\n",
            "iteration 3200 loss: 982.445068359375\n",
            "iteration 3250 loss: 978.1431884765625\n",
            "iteration 3300 loss: 973.9675903320312\n",
            "iteration 3350 loss: 969.9144287109375\n",
            "iteration 3400 loss: 965.978515625\n",
            "iteration 3450 loss: 962.1537475585938\n",
            "iteration 3500 loss: 958.43603515625\n",
            "iteration 3550 loss: 954.8226928710938\n",
            "iteration 3600 loss: 951.310791015625\n",
            "iteration 3650 loss: 947.8972778320312\n",
            "iteration 3700 loss: 944.5794067382812\n",
            "iteration 3750 loss: 941.3515625\n",
            "iteration 3800 loss: 938.2125244140625\n",
            "iteration 3850 loss: 935.1586303710938\n",
            "iteration 3900 loss: 932.185302734375\n",
            "iteration 3950 loss: 929.2893676757812\n",
            "iteration 4000 loss: 926.47119140625\n",
            "iteration 50 loss: 6061.43408203125\n",
            "iteration 100 loss: 4376.71923828125\n",
            "iteration 150 loss: 3617.11181640625\n",
            "iteration 200 loss: 3161.720458984375\n",
            "iteration 250 loss: 2849.09375\n",
            "iteration 300 loss: 2616.235107421875\n",
            "iteration 350 loss: 2433.752197265625\n",
            "iteration 400 loss: 2286.2880859375\n",
            "iteration 450 loss: 2163.82568359375\n",
            "iteration 500 loss: 2060.36572265625\n",
            "iteration 550 loss: 1971.18212890625\n",
            "iteration 600 loss: 1893.6610107421875\n",
            "iteration 650 loss: 1825.631591796875\n",
            "iteration 700 loss: 1765.1900634765625\n",
            "iteration 750 loss: 1711.1123046875\n",
            "iteration 800 loss: 1662.4735107421875\n",
            "iteration 850 loss: 1618.360595703125\n",
            "iteration 900 loss: 1578.02880859375\n",
            "iteration 950 loss: 1541.14208984375\n",
            "iteration 1000 loss: 1507.251953125\n",
            "iteration 1050 loss: 1475.9114990234375\n",
            "iteration 1100 loss: 1446.9293212890625\n",
            "iteration 1150 loss: 1420.0361328125\n",
            "iteration 1200 loss: 1394.916015625\n",
            "iteration 1250 loss: 1371.43359375\n",
            "iteration 1300 loss: 1349.4442138671875\n",
            "iteration 1350 loss: 1328.8192138671875\n",
            "iteration 1400 loss: 1309.4425048828125\n",
            "iteration 1450 loss: 1291.1817626953125\n",
            "iteration 1500 loss: 1273.964111328125\n",
            "iteration 1550 loss: 1257.69921875\n",
            "iteration 1600 loss: 1242.2974853515625\n",
            "iteration 1650 loss: 1227.6790771484375\n",
            "iteration 1700 loss: 1213.790771484375\n",
            "iteration 1750 loss: 1200.5858154296875\n",
            "iteration 1800 loss: 1188.0010986328125\n",
            "iteration 1850 loss: 1176.004638671875\n",
            "iteration 1900 loss: 1164.5517578125\n",
            "iteration 1950 loss: 1153.60498046875\n",
            "iteration 2000 loss: 1143.1376953125\n",
            "iteration 2050 loss: 1133.1171875\n",
            "iteration 2100 loss: 1123.5057373046875\n",
            "iteration 2150 loss: 1114.27587890625\n",
            "iteration 2200 loss: 1105.4111328125\n",
            "iteration 2250 loss: 1096.8988037109375\n",
            "iteration 2300 loss: 1088.7198486328125\n",
            "iteration 2350 loss: 1080.8489990234375\n",
            "iteration 2400 loss: 1073.2747802734375\n",
            "iteration 2450 loss: 1065.9730224609375\n",
            "iteration 2500 loss: 1058.9307861328125\n",
            "iteration 2550 loss: 1052.1329345703125\n",
            "iteration 2600 loss: 1045.5679931640625\n",
            "iteration 2650 loss: 1039.227783203125\n",
            "iteration 2700 loss: 1033.09716796875\n",
            "iteration 2750 loss: 1027.1708984375\n",
            "iteration 2800 loss: 1021.4407958984375\n",
            "iteration 2850 loss: 1015.8993530273438\n",
            "iteration 2900 loss: 1010.5380859375\n",
            "iteration 2950 loss: 1005.349609375\n",
            "iteration 3000 loss: 1000.3255615234375\n",
            "iteration 3050 loss: 995.4623413085938\n",
            "iteration 3100 loss: 990.7509765625\n",
            "iteration 3150 loss: 986.1841430664062\n",
            "iteration 3200 loss: 981.7581176757812\n",
            "iteration 3250 loss: 977.4627075195312\n",
            "iteration 3300 loss: 973.2936401367188\n",
            "iteration 3350 loss: 969.2493286132812\n",
            "iteration 3400 loss: 965.3222045898438\n",
            "iteration 3450 loss: 961.508544921875\n",
            "iteration 3500 loss: 957.8037109375\n",
            "iteration 3550 loss: 954.202880859375\n",
            "iteration 3600 loss: 950.7030029296875\n",
            "iteration 3650 loss: 947.3013916015625\n",
            "iteration 3700 loss: 943.991943359375\n",
            "iteration 3750 loss: 940.7734375\n",
            "iteration 3800 loss: 937.6427612304688\n",
            "iteration 3850 loss: 934.598388671875\n",
            "iteration 3900 loss: 931.6372680664062\n",
            "iteration 3950 loss: 928.7553100585938\n",
            "iteration 4000 loss: 925.950927734375\n",
            "iteration 50 loss: 1896.7318115234375\n",
            "iteration 100 loss: 1466.4754638671875\n",
            "iteration 150 loss: 1265.4788818359375\n",
            "iteration 200 loss: 1141.0792236328125\n",
            "iteration 250 loss: 1053.884765625\n",
            "iteration 300 loss: 988.1790771484375\n",
            "iteration 350 loss: 936.3875732421875\n",
            "iteration 400 loss: 894.3043823242188\n",
            "iteration 450 loss: 859.3433837890625\n",
            "iteration 500 loss: 829.5322265625\n",
            "iteration 550 loss: 803.7687377929688\n",
            "iteration 600 loss: 781.245361328125\n",
            "iteration 650 loss: 761.3492431640625\n",
            "iteration 700 loss: 743.6353149414062\n",
            "iteration 750 loss: 727.7329711914062\n",
            "iteration 800 loss: 713.3521118164062\n",
            "iteration 850 loss: 700.2572021484375\n",
            "iteration 900 loss: 688.26025390625\n",
            "iteration 950 loss: 677.2173461914062\n",
            "iteration 1000 loss: 667.0338745117188\n",
            "iteration 1050 loss: 657.6151733398438\n",
            "iteration 1100 loss: 648.8812255859375\n",
            "iteration 1150 loss: 640.741943359375\n",
            "iteration 1200 loss: 633.1487426757812\n",
            "iteration 1250 loss: 626.0493774414062\n",
            "iteration 1300 loss: 619.3919677734375\n",
            "iteration 1350 loss: 613.12353515625\n",
            "iteration 1400 loss: 607.2260131835938\n",
            "iteration 1450 loss: 601.6500244140625\n",
            "iteration 1500 loss: 596.3876953125\n",
            "iteration 1550 loss: 591.40966796875\n",
            "iteration 1600 loss: 586.6734619140625\n",
            "iteration 1650 loss: 582.1710815429688\n",
            "iteration 1700 loss: 577.8892211914062\n",
            "iteration 1750 loss: 573.8165283203125\n",
            "iteration 1800 loss: 569.9248046875\n",
            "iteration 1850 loss: 566.2235717773438\n",
            "iteration 1900 loss: 562.6935424804688\n",
            "iteration 1950 loss: 559.3070678710938\n",
            "iteration 2000 loss: 556.07275390625\n",
            "iteration 2050 loss: 552.9739379882812\n",
            "iteration 2100 loss: 550.0013427734375\n",
            "iteration 2150 loss: 547.1590576171875\n",
            "iteration 2200 loss: 544.4306030273438\n",
            "iteration 2250 loss: 541.8134155273438\n",
            "iteration 2300 loss: 539.3017578125\n",
            "iteration 2350 loss: 536.8943481445312\n",
            "iteration 2400 loss: 534.5783081054688\n",
            "iteration 2450 loss: 532.3470458984375\n",
            "iteration 2500 loss: 530.1995239257812\n",
            "iteration 2550 loss: 528.1273803710938\n",
            "iteration 2600 loss: 526.1305541992188\n",
            "iteration 2650 loss: 524.2000122070312\n",
            "iteration 2700 loss: 522.3380737304688\n",
            "iteration 2750 loss: 520.5406494140625\n",
            "iteration 2800 loss: 518.8070678710938\n",
            "iteration 2850 loss: 517.1289672851562\n",
            "iteration 2900 loss: 515.5060424804688\n",
            "iteration 2950 loss: 513.9332275390625\n",
            "iteration 3000 loss: 512.4108276367188\n",
            "iteration 3050 loss: 510.9364929199219\n",
            "iteration 3100 loss: 509.5059509277344\n",
            "iteration 3150 loss: 508.1159362792969\n",
            "iteration 3200 loss: 506.7662353515625\n",
            "iteration 3250 loss: 505.4588317871094\n",
            "iteration 3300 loss: 504.1922302246094\n",
            "iteration 3350 loss: 502.9670104980469\n",
            "iteration 3400 loss: 501.777099609375\n",
            "iteration 3450 loss: 500.6235656738281\n",
            "iteration 3500 loss: 499.5038757324219\n",
            "iteration 3550 loss: 498.41558837890625\n",
            "iteration 3600 loss: 497.35992431640625\n",
            "iteration 3650 loss: 496.33380126953125\n",
            "iteration 3700 loss: 495.3369445800781\n",
            "iteration 3750 loss: 494.36614990234375\n",
            "iteration 3800 loss: 493.4228515625\n",
            "iteration 3850 loss: 492.5035705566406\n",
            "iteration 3900 loss: 491.6102294921875\n",
            "iteration 3950 loss: 490.7413330078125\n",
            "iteration 4000 loss: 489.89862060546875\n",
            "iteration 50 loss: 575.3215942382812\n",
            "iteration 100 loss: 441.5133972167969\n",
            "iteration 150 loss: 382.9728088378906\n",
            "iteration 200 loss: 348.2673645019531\n",
            "iteration 250 loss: 324.3551330566406\n",
            "iteration 300 loss: 306.3887939453125\n",
            "iteration 350 loss: 292.18621826171875\n",
            "iteration 400 loss: 280.6614990234375\n",
            "iteration 450 loss: 271.0422668457031\n",
            "iteration 500 loss: 262.869384765625\n",
            "iteration 550 loss: 255.7869415283203\n",
            "iteration 600 loss: 249.5697021484375\n",
            "iteration 650 loss: 244.0468292236328\n",
            "iteration 700 loss: 239.11318969726562\n",
            "iteration 750 loss: 234.65289306640625\n",
            "iteration 800 loss: 230.59881591796875\n",
            "iteration 850 loss: 226.91757202148438\n",
            "iteration 900 loss: 223.5345458984375\n",
            "iteration 950 loss: 220.41531372070312\n",
            "iteration 1000 loss: 217.53619384765625\n",
            "iteration 1050 loss: 214.8614501953125\n",
            "iteration 1100 loss: 212.36688232421875\n",
            "iteration 1150 loss: 210.03240966796875\n",
            "iteration 1200 loss: 207.8500213623047\n",
            "iteration 1250 loss: 205.79751586914062\n",
            "iteration 1300 loss: 203.87335205078125\n",
            "iteration 1350 loss: 202.06361389160156\n",
            "iteration 1400 loss: 200.35247802734375\n",
            "iteration 1450 loss: 198.74359130859375\n",
            "iteration 1500 loss: 197.21853637695312\n",
            "iteration 1550 loss: 195.77374267578125\n",
            "iteration 1600 loss: 194.40170288085938\n",
            "iteration 1650 loss: 193.101806640625\n",
            "iteration 1700 loss: 191.85848999023438\n",
            "iteration 1750 loss: 190.66903686523438\n",
            "iteration 1800 loss: 189.5340576171875\n",
            "iteration 1850 loss: 188.44631958007812\n",
            "iteration 1900 loss: 187.40432739257812\n",
            "iteration 1950 loss: 186.40643310546875\n",
            "iteration 2000 loss: 185.44937133789062\n",
            "iteration 2050 loss: 184.53125\n",
            "iteration 2100 loss: 183.65274047851562\n",
            "iteration 2150 loss: 182.8095245361328\n",
            "iteration 2200 loss: 181.9996337890625\n",
            "iteration 2250 loss: 181.2191162109375\n",
            "iteration 2300 loss: 180.47027587890625\n",
            "iteration 2350 loss: 179.74652099609375\n",
            "iteration 2400 loss: 179.04925537109375\n",
            "iteration 2450 loss: 178.37564086914062\n",
            "iteration 2500 loss: 177.7283172607422\n",
            "iteration 2550 loss: 177.10304260253906\n",
            "iteration 2600 loss: 176.50137329101562\n",
            "iteration 2650 loss: 175.9204864501953\n",
            "iteration 2700 loss: 175.3583984375\n",
            "iteration 2750 loss: 174.8154754638672\n",
            "iteration 2800 loss: 174.29051208496094\n",
            "iteration 2850 loss: 173.7825469970703\n",
            "iteration 2900 loss: 173.29217529296875\n",
            "iteration 2950 loss: 172.81617736816406\n",
            "iteration 3000 loss: 172.35585021972656\n",
            "iteration 3050 loss: 171.9099884033203\n",
            "iteration 3100 loss: 171.4770050048828\n",
            "iteration 3150 loss: 171.05796813964844\n",
            "iteration 3200 loss: 170.6502227783203\n",
            "iteration 3250 loss: 170.25559997558594\n",
            "iteration 3300 loss: 169.87095642089844\n",
            "iteration 3350 loss: 169.4980010986328\n",
            "iteration 3400 loss: 169.13597106933594\n",
            "iteration 3450 loss: 168.7840576171875\n",
            "iteration 3500 loss: 168.44140625\n",
            "iteration 3550 loss: 168.10882568359375\n",
            "iteration 3600 loss: 167.78590393066406\n",
            "iteration 3650 loss: 167.4715576171875\n",
            "iteration 3700 loss: 167.16648864746094\n",
            "iteration 3750 loss: 166.86996459960938\n",
            "iteration 3800 loss: 166.58065795898438\n",
            "iteration 3850 loss: 166.29913330078125\n",
            "iteration 3900 loss: 166.02462768554688\n",
            "iteration 3950 loss: 165.7584228515625\n",
            "iteration 4000 loss: 165.49838256835938\n",
            "iteration 50 loss: 115.2615966796875\n",
            "iteration 100 loss: 105.50675201416016\n",
            "iteration 150 loss: 98.56774139404297\n",
            "iteration 200 loss: 93.14837646484375\n",
            "iteration 250 loss: 88.7176284790039\n",
            "iteration 300 loss: 85.0219497680664\n",
            "iteration 350 loss: 81.89002990722656\n",
            "iteration 400 loss: 79.19248962402344\n",
            "iteration 450 loss: 76.8567123413086\n",
            "iteration 500 loss: 74.80374908447266\n",
            "iteration 550 loss: 73.00020599365234\n",
            "iteration 600 loss: 71.40043640136719\n",
            "iteration 650 loss: 69.97843170166016\n",
            "iteration 700 loss: 68.70841217041016\n",
            "iteration 750 loss: 67.55828094482422\n",
            "iteration 800 loss: 66.51457977294922\n",
            "iteration 850 loss: 65.56290435791016\n",
            "iteration 900 loss: 64.69593811035156\n",
            "iteration 950 loss: 63.90263366699219\n",
            "iteration 1000 loss: 63.17105484008789\n",
            "iteration 1050 loss: 62.49375534057617\n",
            "iteration 1100 loss: 61.866146087646484\n",
            "iteration 1150 loss: 61.28255844116211\n",
            "iteration 1200 loss: 60.73744583129883\n",
            "iteration 1250 loss: 60.2292366027832\n",
            "iteration 1300 loss: 59.75326156616211\n",
            "iteration 1350 loss: 59.30582809448242\n",
            "iteration 1400 loss: 58.884674072265625\n",
            "iteration 1450 loss: 58.486061096191406\n",
            "iteration 1500 loss: 58.10926818847656\n",
            "iteration 1550 loss: 57.75105285644531\n",
            "iteration 1600 loss: 57.4116096496582\n",
            "iteration 1650 loss: 57.08887481689453\n",
            "iteration 1700 loss: 56.782012939453125\n",
            "iteration 1750 loss: 56.4890251159668\n",
            "iteration 1800 loss: 56.20956039428711\n",
            "iteration 1850 loss: 55.9432487487793\n",
            "iteration 1900 loss: 55.68846130371094\n",
            "iteration 1950 loss: 55.444786071777344\n",
            "iteration 2000 loss: 55.211021423339844\n",
            "iteration 2050 loss: 54.98725891113281\n",
            "iteration 2100 loss: 54.77275848388672\n",
            "iteration 2150 loss: 54.56776428222656\n",
            "iteration 2200 loss: 54.370941162109375\n",
            "iteration 2250 loss: 54.182254791259766\n",
            "iteration 2300 loss: 54.00096893310547\n",
            "iteration 2350 loss: 53.82642364501953\n",
            "iteration 2400 loss: 53.65834045410156\n",
            "iteration 2450 loss: 53.49618148803711\n",
            "iteration 2500 loss: 53.34019470214844\n",
            "iteration 2550 loss: 53.189659118652344\n",
            "iteration 2600 loss: 53.04360580444336\n",
            "iteration 2650 loss: 52.90267562866211\n",
            "iteration 2700 loss: 52.76683044433594\n",
            "iteration 2750 loss: 52.635623931884766\n",
            "iteration 2800 loss: 52.50828552246094\n",
            "iteration 2850 loss: 52.38489532470703\n",
            "iteration 2900 loss: 52.26543426513672\n",
            "iteration 2950 loss: 52.14994812011719\n",
            "iteration 3000 loss: 52.038204193115234\n",
            "iteration 3050 loss: 51.92974090576172\n",
            "iteration 3100 loss: 51.824676513671875\n",
            "iteration 3150 loss: 51.72309112548828\n",
            "iteration 3200 loss: 51.624568939208984\n",
            "iteration 3250 loss: 51.52897262573242\n",
            "iteration 3300 loss: 51.436004638671875\n",
            "iteration 3350 loss: 51.34587097167969\n",
            "iteration 3400 loss: 51.25837326049805\n",
            "iteration 3450 loss: 51.17329788208008\n",
            "iteration 3500 loss: 51.0906982421875\n",
            "iteration 3550 loss: 51.01057434082031\n",
            "iteration 3600 loss: 50.93277359008789\n",
            "iteration 3650 loss: 50.85710525512695\n",
            "iteration 3700 loss: 50.78335952758789\n",
            "iteration 3750 loss: 50.711669921875\n",
            "iteration 3800 loss: 50.64195251464844\n",
            "iteration 3850 loss: 50.574161529541016\n",
            "iteration 3900 loss: 50.508094787597656\n",
            "iteration 3950 loss: 50.443824768066406\n",
            "iteration 4000 loss: 50.38134002685547\n",
            "iteration 50 loss: 13.051652908325195\n",
            "iteration 100 loss: 12.910183906555176\n",
            "iteration 150 loss: 12.786885261535645\n",
            "iteration 200 loss: 12.675955772399902\n",
            "iteration 250 loss: 12.575342178344727\n",
            "iteration 300 loss: 12.483240127563477\n",
            "iteration 350 loss: 12.39826774597168\n",
            "iteration 400 loss: 12.319540023803711\n",
            "iteration 450 loss: 12.246170043945312\n",
            "iteration 500 loss: 12.177508354187012\n",
            "iteration 550 loss: 12.113188743591309\n",
            "iteration 600 loss: 12.05273151397705\n",
            "iteration 650 loss: 11.99573040008545\n",
            "iteration 700 loss: 11.941795349121094\n",
            "iteration 750 loss: 11.89061450958252\n",
            "iteration 800 loss: 11.84196662902832\n",
            "iteration 850 loss: 11.795671463012695\n",
            "iteration 900 loss: 11.751598358154297\n",
            "iteration 950 loss: 11.70956039428711\n",
            "iteration 1000 loss: 11.669381141662598\n",
            "iteration 1050 loss: 11.630955696105957\n",
            "iteration 1100 loss: 11.594170570373535\n",
            "iteration 1150 loss: 11.558889389038086\n",
            "iteration 1200 loss: 11.525026321411133\n",
            "iteration 1250 loss: 11.492457389831543\n",
            "iteration 1300 loss: 11.461119651794434\n",
            "iteration 1350 loss: 11.430959701538086\n",
            "iteration 1400 loss: 11.401956558227539\n",
            "iteration 1450 loss: 11.373997688293457\n",
            "iteration 1500 loss: 11.347070693969727\n",
            "iteration 1550 loss: 11.321104049682617\n",
            "iteration 1600 loss: 11.296005249023438\n",
            "iteration 1650 loss: 11.271747589111328\n",
            "iteration 1700 loss: 11.248292922973633\n",
            "iteration 1750 loss: 11.225605010986328\n",
            "iteration 1800 loss: 11.203653335571289\n",
            "iteration 1850 loss: 11.182403564453125\n",
            "iteration 1900 loss: 11.161831855773926\n",
            "iteration 1950 loss: 11.141912460327148\n",
            "iteration 2000 loss: 11.122613906860352\n",
            "iteration 2050 loss: 11.103914260864258\n",
            "iteration 2100 loss: 11.085776329040527\n",
            "iteration 2150 loss: 11.068171501159668\n",
            "iteration 2200 loss: 11.051100730895996\n",
            "iteration 2250 loss: 11.034552574157715\n",
            "iteration 2300 loss: 11.018486022949219\n",
            "iteration 2350 loss: 11.002886772155762\n",
            "iteration 2400 loss: 10.987751960754395\n",
            "iteration 2450 loss: 10.97303581237793\n",
            "iteration 2500 loss: 10.958718299865723\n",
            "iteration 2550 loss: 10.944772720336914\n",
            "iteration 2600 loss: 10.931219100952148\n",
            "iteration 2650 loss: 10.9180269241333\n",
            "iteration 2700 loss: 10.90521240234375\n",
            "iteration 2750 loss: 10.892740249633789\n",
            "iteration 2800 loss: 10.880603790283203\n",
            "iteration 2850 loss: 10.868768692016602\n",
            "iteration 2900 loss: 10.857234954833984\n",
            "iteration 2950 loss: 10.845996856689453\n",
            "iteration 3000 loss: 10.835060119628906\n",
            "iteration 3050 loss: 10.824424743652344\n",
            "iteration 3100 loss: 10.81405258178711\n",
            "iteration 3150 loss: 10.80394458770752\n",
            "iteration 3200 loss: 10.794093132019043\n",
            "iteration 3250 loss: 10.784488677978516\n",
            "iteration 3300 loss: 10.775125503540039\n",
            "iteration 3350 loss: 10.766002655029297\n",
            "iteration 3400 loss: 10.757105827331543\n",
            "iteration 3450 loss: 10.748437881469727\n",
            "iteration 3500 loss: 10.739985466003418\n",
            "iteration 3550 loss: 10.73173999786377\n",
            "iteration 3600 loss: 10.723695755004883\n",
            "iteration 3650 loss: 10.715841293334961\n",
            "iteration 3700 loss: 10.708168029785156\n",
            "iteration 3750 loss: 10.700674057006836\n",
            "iteration 3800 loss: 10.69336223602295\n",
            "iteration 3850 loss: 10.686229705810547\n",
            "iteration 3900 loss: 10.679266929626465\n",
            "iteration 3950 loss: 10.672470092773438\n",
            "iteration 4000 loss: 10.665830612182617\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}