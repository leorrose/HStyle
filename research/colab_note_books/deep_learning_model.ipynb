{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYMYk6yPUv79"
      },
      "source": [
        "# Historical style generator\n",
        "##### Students – Leor Ariel Rose, Yahav Bar David\n",
        "##### Academic advisor - Dr. Irina Rabaev\n",
        "\n",
        "This notebook contains our document style tranfer model and the explanations of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kGoL1qs8qWt"
      },
      "source": [
        "First we will import all necessary libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPY561rZa7H"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import List, Union, Tuple, Dict\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from PIL import Image\n",
        "from shutil import copyfile\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-xNBMq8ZkS7"
      },
      "source": [
        "Next we will define our document style transfer class (every method and piece of code is explained by comments and docstrings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMbymj86AoDv"
      },
      "source": [
        "class DocumentStyleTransfer():\n",
        "    totalVariationWeight:float = 0\n",
        "    styleWeight:float = 0\n",
        "    contentWeight:float = 0\n",
        "    imgNumRows:int = 0\n",
        "    imgNumCols:int = 0\n",
        "    ftm:tf.keras.Model = None\n",
        "\n",
        "    def _preprocessImage(self, imagePath:str) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Private Method to open, resize and format pictures into appropriate tensors\n",
        "\n",
        "        Args:\n",
        "            image_path (str): the path to image\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: a tensor representing image\n",
        "        \"\"\"\n",
        "        # load image with content image size\n",
        "        img:Image.Image  = keras.preprocessing.image.load_img(imagePath, target_size=(self.imgNumRows, self.imgNumCols))\n",
        "        # create numpy array of image\n",
        "        img:np.ndarray = keras.preprocessing.image.img_to_array(img)\n",
        "        # expand to 3d for model\n",
        "        img:np.ndarray = np.expand_dims(img, axis=0)\n",
        "        # convert from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
        "        img:Union[np.ndarray,tf.Tensor] = vgg19.preprocess_input(img)\n",
        "        return tf.convert_to_tensor(img)\n",
        "\n",
        "\n",
        "    def _deprocessImage(self, tns:tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Private Method to convert a tensor into a valid image\n",
        "\n",
        "        Args:\n",
        "            tns (tf.Tensor): a tensor from model output\n",
        "\n",
        "        Returns:\n",
        "            [tf.Tensor]: a tensor that represents an image\n",
        "        \"\"\"\n",
        "        # reshape to 2d instead of 3d from model output: '(1, 400, 571, 3)' -> '(400, 571, 3)'\n",
        "        tns:tf.tensor = tns.reshape((self.imgNumRows, self.imgNumCols, 3))\n",
        "                \n",
        "        # Remove zero-center by mean pixel\n",
        "        tns[:, :, 0] += 103.939\n",
        "        tns[:, :, 1] += 116.779\n",
        "        tns[:, :, 2] += 123.68\n",
        "                \n",
        "        # 'BGR'->'RGB'\n",
        "        tns:tf.tensor = tns[:, :, ::-1]\n",
        "        tns:tf.tensor = np.clip(tns, 0, 255).astype(\"uint8\")\n",
        "                \n",
        "        # return valid tensor image\n",
        "        return tns\n",
        "\n",
        "    def _gramMatrix(self, tns:tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Private Method to calculate gram matrix of an image tensor (feature-wise outer product)\n",
        "\n",
        "        Args:\n",
        "            tns (tf.Tensor): a tensor that represents an image\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: the _gramMatrix result\n",
        "        \"\"\"\n",
        "        # transpose tensor by (2,0,1) for example tensor shape (400, 571, 64)' -> '(64, 400, 571)'\n",
        "        tns:tf.tensor = tf.transpose(tns, (2, 0, 1))\n",
        "        # flatten to 2d for example tensor shape '(64, 400, 571)' -> '(64, 228400)'\n",
        "        features:tf.tensor = tf.reshape(tns, (tf.shape(tns)[0], -1))\n",
        "        # matrix multiplication of features and features transpose\n",
        "        gram:tf.tensor = tf.matmul(features, tf.transpose(features))\n",
        "        return gram\n",
        "\n",
        "    def _styleLoss(self, style:tf.Tensor, combination:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Method to calculate the style loss. \n",
        "        The \"style loss\" is designed to maintain the style of the reference image in the generated image.\n",
        "        It is contentd on the gram matrices (which capture style) of feature maps from the style reference image\n",
        "        and from the generated image.\n",
        "\n",
        "        Args:\n",
        "            style (tf.Tensor): tensor representing style image\n",
        "            combination (tf.Tensor): tensor representing combination image\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        # calc gram matrix for style\n",
        "        style_gramMatrix:tf.tensor = self._gramMatrix(style)\n",
        "        # calc gram matrix for combination\n",
        "        combination_gramMatrix:tf.tensor = self._gramMatrix(combination)\n",
        "        # image depth\n",
        "        channels:int = 3\n",
        "        # image size\n",
        "        size:int = self.imgNumRows * self.imgNumCols\n",
        "        # MSE loss between gram matrix of input and the style image\n",
        "        return tf.reduce_sum(tf.square(style_gramMatrix - combination_gramMatrix)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "\n",
        "    def _contentLoss(self, content:tf.Tensor, combination:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Method to calculate the content loss. \n",
        "        An auxiliary loss function designed to maintain the \"content\" of the\n",
        "        content image in the generated image.\n",
        "\n",
        "        Args:\n",
        "            content (tf.Tensor): tensor representing the content content image\n",
        "            combination (tf.Tensor): tensor representing combination image\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        # MSE loss between the content content image’s features and the combination image’s features\n",
        "        return tf.reduce_sum(tf.square(combination - content))\n",
        "        \n",
        "    def _totalVariationLoss(self, tns:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Methos to calculate total variation loss (a regularization loss), designed to keep the generated image locally coherent.\n",
        "\n",
        "        Args:\n",
        "            tns (tf.Tensor):  tensor representing the generated image\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        a:tf.Tensor = tf.square(tns[:, : self.imgNumRows - 1, : self.imgNumCols - 1, :] - tns[:, 1:, : self.imgNumCols - 1, :])\n",
        "        b:tf.Tensor = tf.square(tns[:, : self.imgNumRows - 1, : self.imgNumCols - 1, :] - tns[:, : self.imgNumRows - 1, 1:, :])\n",
        "        return tf.reduce_sum(tf.pow(a + b, 1.25))\n",
        "\n",
        "    def _featureExtractorModel(self) -> tf.keras.Model:\n",
        "        \"\"\"\n",
        "        Private Method to create a model that returns the activation values for every layer in VGG19 (as a dict)\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: a model that returns the activation values for every layer in VGG19 (as a dict)\n",
        "        \"\"\"\n",
        "        # Build a VGG19 model loaded with pre-trained ImageNet weights\n",
        "        model:tf.keras.Model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "        # Get the symbolic outputs of each \"key\" layer (they have unique names)\n",
        "        outputsDict:Dict[str, tf.Tensor] = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "\n",
        "        # Set up a model that returns the activation values for every layer in VGG19 (as a dict).\n",
        "        featureExtractor:tf.keras.Model = keras.Model(inputs=model.inputs, outputs=outputsDict)\n",
        "        return featureExtractor\n",
        "\n",
        "    def _computeLoss(self, combinationImage:tf.Tensor, contentImage:tf.Tensor, styleReferenceImage:tf.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Private Method to combine style, content, total variation loss functions  into one loss function for model evaluation.\n",
        "\n",
        "        Args:\n",
        "            combinationImage (tf.Tensor): \n",
        "            contentImage (tf.Tensor): \n",
        "            styleReferenceImage (tf.Tensor):\n",
        "\n",
        "        Returns:\n",
        "            float: the loss value\n",
        "        \"\"\"\n",
        "        # List of layers to use for the style loss.\n",
        "        styleLayerNames:List[str] = [\"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\", \"block5_conv1\"]\n",
        "        \n",
        "        # The layer to use for the content loss.\n",
        "        contentLayerName:str = \"block5_conv2\"\n",
        "\n",
        "        # concat all images to one for model prediction\n",
        "        inputTensor:tf.Tensor = tf.concat([contentImage, styleReferenceImage, combinationImage], axis=0)\n",
        "            \n",
        "        # get prediction from model with input image as a dict of each layer\n",
        "        features:Dict[str, tf.Tensor]  = self.ftm(inputTensor)\n",
        "\n",
        "        # Initialize the loss\n",
        "        loss:np.ndarray = tf.zeros(shape=())\n",
        "\n",
        "        # get content layer\n",
        "        layerFeatures = features[contentLayerName] \n",
        "        # get content image features from layer prediction\n",
        "        contentImageFeatures = layerFeatures[0, :, :, :]\n",
        "        # get combination image features from layer prediction\n",
        "        combinationFeatures = layerFeatures[2, :, :, :]\n",
        "        # add content loss to total loss\n",
        "        loss:np.ndarray = loss + self.contentWeight * self._contentLoss(contentImageFeatures, combinationFeatures)\n",
        "\n",
        "        # Add style loss\n",
        "        for layerName in styleLayerNames:\n",
        "            # get style layer \n",
        "            layerFeatures = features[layerName]\n",
        "            # get style image features from layer prediction\n",
        "            styleReferenceFeatures = layerFeatures[1, :, :, :]\n",
        "            # get combination image features from layer prediction\n",
        "            combinationFeatures = layerFeatures[2, :, :, :]\n",
        "            # add style loss to total loss\n",
        "            loss += (self.styleWeight / len(styleLayerNames)) * self._styleLoss(styleReferenceFeatures, combinationFeatures)\n",
        "\n",
        "        # Add total variation loss\n",
        "        # loss += self.totalVariationWeight * self._totalVariationLoss(combinationImage)\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def _computeLossAndGrads(self, combinationImage:tf.Tensor, contentImage:tf.Tensor, styleReferenceImage:tf.Tensor) -> Tuple[float, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Private Method to compile loss function to make it faster.\n",
        "\n",
        "        Args:\n",
        "            combinationImage (tf.Tensor): \n",
        "            contentImage (tf.Tensor): \n",
        "            styleReferenceImage (tf.Tensor): \n",
        "\n",
        "        Returns:\n",
        "            Tuple[float, tf.Tensor]: \n",
        "        \"\"\"\n",
        "        # \n",
        "        with tf.GradientTape() as tape:\n",
        "            #\n",
        "            loss:float = self._computeLoss(combinationImage, contentImage, styleReferenceImage)\n",
        "        #\n",
        "        grads:tf.Tensor = tape.gradient(loss, combinationImage)\n",
        "        return loss, grads\n",
        "\n",
        "    def renderImage(self, contentImgPath:str, styleImgPath:str, resultPrefix:str = 'result', resultsDir:str = './', \n",
        "                    totalVariationWeight:float = 1e-6, styleWeight:float = 1e-6, contentWeight:float = 2.5e-8, iterSave:int = 4000) -> None:\n",
        "        \"\"\"\n",
        "        Method to render a style transfer image from content and style\n",
        "\n",
        "        Args:\n",
        "            contentImgPath (str): our content image path\n",
        "            styleImgPath (str): our style image path\n",
        "            resultPrefix (str, optional): the result prefix for saving images and content. Defaults to 'result-'.\n",
        "            resultsDir (str, optional): directory to save content. Defaults to './'.\n",
        "            totalVariationWeight (float, optional): total variation loss weight. Defaults to 1e-6.\n",
        "            styleWeight (float, optional): style loss weight. Defaults to 1e-6.\n",
        "            contentWeight (float, optional): content loss weight. Defaults to 2.5e-8.\n",
        "            iterSave (int, optional): iteration difference to save image (each itersave it will save result image). Defaults to 4000 (one image).\n",
        "        \"\"\"\n",
        "        # create directory for results\n",
        "        if os.path.exists(resultsDir):\n",
        "            shutil.rmtree(resultsDir)\n",
        "        os.mkdir(resultsDir)\n",
        "\n",
        "        # Weights of the different loss components\n",
        "        self.totalVariationWeight:float = totalVariationWeight\n",
        "        self.styleWeight:float = styleWeight\n",
        "        self.contentWeight:float = contentWeight\n",
        "\n",
        "        # Dimensions of the generated picture.\n",
        "        width, height = keras.preprocessing.image.load_img(contentImgPath).size\n",
        "        self.imgNumCols:int = 400\n",
        "        self.imgNumRows:int = int(width * self.imgNumCols / height)\n",
        "\n",
        "        # create feature extractor model\n",
        "        self.ftm = self._featureExtractorModel()\n",
        "\n",
        "        # set model optimizer\n",
        "        optimizer = keras.optimizers.SGD(keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96))\n",
        "\n",
        "        # get content image\n",
        "        contentImage = self._preprocessImage(contentImgPath)\n",
        "        # get style image\n",
        "        styleReferenceImage = self._preprocessImage(styleImgPath)\n",
        "        # create combination initial image\n",
        "        combinationImage = tf.Variable(self._preprocessImage(contentImgPath))\n",
        "\n",
        "        # set number of iteration of model\n",
        "        iterations = 4000\n",
        "        for i in range(1, iterations + 1):\n",
        "            # compute loss and gradient\n",
        "            loss, grads = self._computeLossAndGrads(combinationImage, contentImage, styleReferenceImage)\n",
        "            # apply the gradient to combination image\n",
        "            optimizer.apply_gradients([(grads, combinationImage)])\n",
        "            # for each 50 iteration ouput results\n",
        "            if i % iterSave == 0:\n",
        "                print(f\"iteration {i} loss: {loss}\")\n",
        "                img = self._deprocessImage(combinationImage.numpy())\n",
        "                fname = resultsDir + '/' + resultPrefix + f\"_at_iteration_{i}.png\"\n",
        "                keras.preprocessing.image.save_img(fname, img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4kL99NJaB50"
      },
      "source": [
        "Next we will define our content and loss values in order to keep a raitio between them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y08zXmdRaLu-"
      },
      "source": [
        "# weights\n",
        "contentWeights: List[float] = [1.00E-06, 1.00E-07, 1.00E-08, 1.00E-09, 1.00E-10, 1.00E-06, 1.00E-06, 1.00E-06, 1.00E-06]\n",
        "styleWeights: List[float] =   [1.00E-06, 1.00E-06, 1.00E-06, 1.00E-06, 1.00E-06, 1.00E-07, 1.00E-08, 1.00E-09, 1.00E-10]\n",
        "totalVariationWeight:float = 1e-06"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZp6rz_iaU4i"
      },
      "source": [
        "Lets create a document style transfer object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNABikukaXc5"
      },
      "source": [
        "# model\n",
        "model:DocumentStyleTransfer = DocumentStyleTransfer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUZbc-qfaYEA"
      },
      "source": [
        "And know for each content and style loss we will apply document style transfer and save the result and our research parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0p3OALAA6_Y"
      },
      "source": [
        "# experements\n",
        "for i in range(0, len(contentWeights)):\n",
        "  model.renderImage('/content/content.png', '/content/style.png', f\"expererment_{i}\", f\"/content/expererment{i}\", totalVariationWeight, styleWeights[i], contentWeights[i], 50)\n",
        "  \n",
        "  # create read me of the experement inside the folder\n",
        "  with open(f\"/content/expererment{i}/readme.txt\", 'w') as readme:\n",
        "    readme.write(f\"experementNumber = {i}\\n\")\n",
        "    readme.write(f\"from = modern hebrew\\n\")\n",
        "    readme.write(f\"to = middle ages hebrew\\n\")\n",
        "    # readme.write(f\"totalVariationWeight = {totalVariationWeight}\\n\")\n",
        "    readme.write(f\"styleWeight = {styleWeights[i]}\\n\")\n",
        "    readme.write(f\"contentWeight = {contentWeights[i]}\\n\")\n",
        "  \n",
        "  # add contetnt and style to experement folder   \n",
        "  copyfile('/content/content.png', f\"/content/expererment{i}/content.png\")\n",
        "  copyfile('/content/style.png', f\"/content/expererment{i}/style.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o90C0gtxauBT"
      },
      "source": [
        "And lastly we will zip and download our experements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9HA3E8HBX1S"
      },
      "source": [
        "!zip -r expererment0.zip /content/expererment0\n",
        "!zip -r expererment1.zip /content/expererment1\n",
        "!zip -r expererment2.zip /content/expererment2\n",
        "!zip -r expererment3.zip /content/expererment3\n",
        "!zip -r expererment4.zip /content/expererment4\n",
        "!zip -r expererment5.zip /content/expererment5\n",
        "!zip -r expererment6.zip /content/expererment6\n",
        "!zip -r expererment7.zip /content/expererment7\n",
        "!zip -r expererment8.zip /content/expererment8\n",
        "\n",
        "\n",
        "for i in range(0,9):\n",
        "  files.download(f\"/content/expererment{i}.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}